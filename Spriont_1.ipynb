{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine learning flow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nklbigone/DIC-ML/blob/First-Assignment/Spriont_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMPAdNGI5rge"
      },
      "source": [
        "**[Problem 1] Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XuVdlgI5mDU",
        "outputId": "0f0c97b3-119a-449b-effe-0d63712fb494"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the csv of the dataset\n",
        "df = pd.read_csv('/application_train.csv')\n",
        "df = df.select_dtypes('number')\n",
        "\n",
        "# cleaning the dataset by filling the empy data(null)\n",
        "cleaned_df = df.fillna(0)\n",
        "\n",
        "# get only existing data with no missing values\n",
        "cleaned_df = cleaned_df[cleaned_df.columns[~cleaned_df.isnull().all()]]\n",
        "\n",
        "# separating them into variables\n",
        "y = cleaned_df['TARGET']\n",
        "X = cleaned_df.drop(['TARGET'], axis=1)\n",
        "\n",
        "X = X.to_numpy()\n",
        "\n",
        "kf = KFold(n_splits=2)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [15541 15542 15543 ... 31078 31079 31080] TEST: [    0     1     2 ... 15538 15539 15540]\n",
            "TRAIN: [    0     1     2 ... 15538 15539 15540] TEST: [15541 15542 15543 ... 31078 31079 31080]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSBFNStODvgw"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_trans = scaler.transform(X_train)\n",
        "X_test_trans = scaler.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WanBR3CWD5xw"
      },
      "source": [
        "\n",
        "**[Problem 2] Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "gNj0i93VECDQ",
        "outputId": "e3cb4c8a-3ea5-43ca-c5ef-494928a0e451"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# checking which model and params are best\n",
        "model_params = {\n",
        "    'random_forest':{\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [1,5,10]\n",
        "        }\n",
        "    },\n",
        "    'logic_regression':{\n",
        "        'model': LogisticRegression(solver=\"liblinear\",multi_class=\"auto\"),\n",
        "        'params': {\n",
        "            'C': [1,5,10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# defining an array to store the scores\n",
        "scores = []\n",
        "\n",
        "for model_name,mp in model_params.items():\n",
        "    clf = GridSearchCV(mp['model'],mp['params'], return_train_score=False)\n",
        "    clf.fit(X_train_trans,y_train)\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "best_model_params = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
        "best_model_params"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>best_score</th>\n",
              "      <th>best_params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>random_forest</td>\n",
              "      <td>0.851164</td>\n",
              "      <td>{'n_estimators': 10}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>logic_regression</td>\n",
              "      <td>0.920983</td>\n",
              "      <td>{'C': 1}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model  best_score           best_params\n",
              "0     random_forest    0.851164  {'n_estimators': 10}\n",
              "1  logic_regression    0.920983              {'C': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYEhcFn8JWdL"
      },
      "source": [
        "**[Problem 3] Survey from Kaggle Notebooks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mkea_L0YxUE"
      },
      "source": [
        "Model Hyperparameter Optimization is the  points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.\n",
        "\n",
        "Parameters are different from hyperparameters. Parameters are learned automatically; hyperparameters are set manually to help guide the learning process.\n",
        "\n",
        "the Gradient Boosting Algorithm for Machine Learning The origin of boosting from learning theory and AdaBoost.\n",
        "How gradient boosting works including the loss function, weak learners and the additive model.\n",
        "How to improve performance over the base algorithm with various regularization schemes.\n",
        "Modal explainability \n",
        "Early Stopping to Avoid Overtraining Neural Networks A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. This simple, effective, and widely used approach to training neural networks is called early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp89x6Faea-g"
      },
      "source": [
        "**[Problem 4] Creating a model with high generalization performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi1oPfs1JY5r",
        "outputId": "cbe5120e-f669-4ba6-d9b0-6b8c80cb5a7a"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# creating an instance of the model\n",
        "model = lgb.LGBMClassifier()\n",
        "\n",
        "# save the default params\n",
        "default_params = model.get_params()\n",
        "\n",
        "# number of folds\n",
        "N_FOLDS = 5\n",
        "\n",
        "# creating a dataset\n",
        "train_set = lgb.Dataset(data = X_train)\n",
        "\n",
        "# Cross validation results when avoid overfitting\n",
        "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, metrics = 'auc', nfold = N_FOLDS, seed = 42)\n",
        "\n",
        "# displaying the results\n",
        "print('The maximum validation ROC AUC was: {:.5f}.'.format(cv_results['auc-mean'][-1]))\n",
        "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:430: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:741: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
            "Please use silent argument of the Dataset constructor to pass this parameter.\n",
            "  .format(key))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The maximum validation ROC AUC was: 1.00000.\n",
            "The optimal number of boosting rounds (estimators) was 1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwv39UifE6w"
      },
      "source": [
        "imported the whole dataset\n",
        "created a subset of only numbers\n",
        "split the data using kfold\n",
        "used gridsearchCV to find the best model and params to fine tune my classiffiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFDwyBnKfhM-"
      },
      "source": [
        "**[Problem 5] Final model selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQHboWkhflVt"
      },
      "source": [
        "\n",
        "# loading the csv of the test dataset\n",
        "test_df = pd.read_csv('/application_test.csv')\n",
        "\n",
        "# cleaning the dataset by removing the empy data(null)\n",
        "test_cleaned_df = test_df.fillna(0)\n",
        "\n",
        "# separating them into variables\n",
        "test_X = test_cleaned_df.select_dtypes('number')\n",
        "\n",
        "# standardizing the data\n",
        "test_scaler = StandardScaler()\n",
        "test_X_test_trans = scaler.fit_transform(test_X)\n",
        "\n",
        "# predicting\n",
        "test_reg_pred = clf.predict(test_X_test_trans)\n",
        "\n",
        "kgl_submission = pd.concat([test_df['SK_ID_CURR'], pd.Series(test_reg_pred, name='TARGET')], axis=1)\n",
        "kgl_submission.to_csv('application_test1.csv ', index=False)"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}